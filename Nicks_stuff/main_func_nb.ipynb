{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b9a76e26",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-02T05:19:21.631641Z",
     "start_time": "2023-10-02T05:19:20.452931Z"
    },
    "cell_style": "center"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "def RF_complete(data_t: \"df\", data_s_list: \"list(df)\", feature_t, n_tree=50, n_feature=5, f_sample=0.3, n_best_tree=5):\n",
    "    '''\n",
    "    TODO:\n",
    "    - data_s_list might be a list of dataframes... Not just a list corresponding to one thing\n",
    "    '''\n",
    "    K = len(data_s_list) # Number of datasets given for training\n",
    "    print(f\"{K} additional datasets used for prediction.\")\n",
    "\n",
    "    if feature_t not in data_t.columns:\n",
    "        print(\"Feature-of-interest not found in data_t! Please check column names of input data.\")\n",
    "        return None\n",
    "    else:\n",
    "        print(\"Feature-of-interest located!\")\n",
    "\n",
    "    mu = []\n",
    "    sigma = []\n",
    "\n",
    "    for k in range(0, K): # Iteratingg over data_s_list\n",
    "        data_assist = data_s_list[k] #.iloc[k-1] # Indexing is in terms of rows\n",
    "        # Locate feature of interest in training dataset k of K\n",
    "        if feature_t in data_assist.columns:\n",
    "            f_t_ind = list(data_assist.columns).index(feature_t) # Finds col/index of current sera that matches virus of interest\n",
    "        else:\n",
    "            print(f\"feature_t not found in assisting data {k}!\")\n",
    "            continue\n",
    "        # Check for feature match\n",
    "#         if not data_t.columns.equals(data_assist.columns): # MISTAKE!!! -> Solved?\n",
    "        if sum(data_t.columns != data_assist.columns) > 0: # Check for any non overlapping viruses between training dataset and prediction data\n",
    "            print(f\"Features not matched for assisting data {k}! Skipped to next data.\")\n",
    "            continue\n",
    "        # Transferability function\n",
    "        trans_true_err = []\n",
    "        trans_pred_err = []\n",
    "        print(f\"data_assist shape: {data_assist.shape}\")\n",
    "        print(f\"Iterating over {data_assist.shape[1]}\")\n",
    "        # Iterate over dataset columns\n",
    "        for j in range(0, data_assist.shape[1]): # Iterate over rows of training data\n",
    "            feature_trans = data_assist.columns[j]\n",
    "#             if not data_t.iloc[:, j].isna().sum() > 0: # Check this for mistake\n",
    "            if data_t.iloc[:, j].notna().sum() > 0:\n",
    "                rf_1t1 = RF_complete_1t1(data_assist, data_t, feature_t=feature_trans, n_tree=n_tree,\n",
    "                                         n_feature=n_feature, f_sample=f_sample, k=k)\n",
    "\n",
    "                if rf_1t1 is not None: # Check to see that rf_1t1 is a val and not None or a bool\n",
    "                    trans_true_err.extend(rf_1t1[\"true_err\"])\n",
    "                    trans_pred_err.extend(rf_1t1[\"pred_err\"])\n",
    "\n",
    "        if len(trans_true_err) == 0:\n",
    "            continue\n",
    "        else: # ooh tough cookie\n",
    "            print(\"Doing lm stuff\")\n",
    "            lm_coeff = np.polyfit(trans_pred_err, trans_true_err, 1)\n",
    "            a = lm_coeff[0]\n",
    "            b = lm_coeff[1]\n",
    "            c = np.sqrt(np.mean((a * np.array(trans_pred_err) + b - np.array(trans_true_err)) ** 2))\n",
    "\n",
    "            def f_transfer(x, a, b, c):\n",
    "                return max(x, a * x + b + c)\n",
    "\n",
    "            print([f\"a={round(a, 3)}\", f\"b={round(b, 3)}\", f\"c={round(c, 3)}\"])\n",
    "\n",
    "            rf_1t1 = RF_complete_1t1(data_assist, data_t, feature_t=feature_t, n_best_tree=n_best_tree, n_tree=n_tree,\n",
    "                                     n_feature=n_feature, f_sample=f_sample, k=k)\n",
    "            print(f\"rf_1t1: {rf_1t1}\")\n",
    "            mu.append(rf_1t1[\"mu\"])\n",
    "            sigma.append(f_transfer(np.mean(rf_1t1[\"pred_err\"]), a, b, c))\n",
    "\n",
    "    A = 0\n",
    "    B = 0\n",
    "    tt = 0\n",
    "    print(f\"len sigma: {len(sigma)}\")\n",
    "    print(sigma)\n",
    "    print(f\"K: {K}\")\n",
    "    for k in range(K):\n",
    "        print(f\"k: {k}\")\n",
    "        if sigma[k] is not None:\n",
    "            tt += 1\n",
    "            A += mu[k] / sigma[k] ** 2\n",
    "            B += 1 / sigma[k] ** 2\n",
    "\n",
    "    print(f\"{tt} assisting data used for prediction.\")\n",
    "    return {\"predictions\": A / B, \"errors\": 1 / np.sqrt(B)}\n",
    "\n",
    "def RF_complete_1t1(data_assist, data_t, feature_t, n_tree=50, n_feature=5, f_sample=0.3, n_best_tree=5, k=1):\n",
    "    if feature_t in data_assist.columns:\n",
    "#         f_t_ind = data_assist.index.get_loc(feature_t)\n",
    "        f_t_ind = list(data_assist.columns).index(feature_t)\n",
    "    else:\n",
    "        print(f\"feature_t not found in assisting data {k}!\")\n",
    "        return None\n",
    "    # IM WORKING ON THIS ... REFERS TO VIRUSES I THINK, NOT SERA. Yeah R code mentions finding feasible features\n",
    "#     if (data_assist.apply(lambda x: x.count(), axis=0) / data_assist.shape[0] > 0.8).sum() > n_feature:\n",
    "#         f_ind = data_assist.index[\n",
    "#             (data_assist.apply(lambda x: x.count(), axis=0) / data_assist.shape[0] > 0.8)].tolist()\n",
    "    if (data_assist.apply(lambda x: x.count(), axis=0) / data_assist.shape[0] > 0.8).sum() > n_feature:\n",
    "        # f_ind should be a list of columns/viruses\n",
    "        f_ind = list(data_assist.columns[(data_assist.apply(lambda x: x.count(), axis=0) / data_assist.shape[0] > 0.8).tolist()])\n",
    "        if feature_t in f_ind:\n",
    "            f_ind.remove(feature_t)\n",
    "    else:\n",
    "        print(f\"n_feature too large for assisting data {k}! Skipped to next data.\")\n",
    "        return None\n",
    "    # Left off here\n",
    "    f_tmp_ind = [data_t.columns.get_loc(f) for f in f_ind if f in data_t.columns]\n",
    "    f_feasible = [f for f in data_t.columns[f_tmp_ind] if data_t[f].count() > 2]\n",
    "    f_ind = [f_ind[idx] for idx in range(len(f_ind)) if f_ind[idx] == f_feasible[idx]] # Added cuz the r code has this after f.feasible asgn\n",
    "    if len(f_feasible) < 2:\n",
    "        print(f\"n_feature too large for assisting data {k}! Skipped to next data.\")\n",
    "        return None\n",
    "\n",
    "    data_assist = data_assist.dropna(subset=[feature_t])\n",
    "\n",
    "    RMSE = []\n",
    "    f_sel_ind = []\n",
    "    tree = []\n",
    "\n",
    "    for i in range(n_tree):\n",
    "        f_sel_ind.append(np.random.choice(f_ind, n_feature, replace=True))\n",
    "        sample_sel = np.random.choice(data_assist.shape[0], int(data_assist.shape[0] * f_sample), replace=True)\n",
    "        # LAST ERROR: Indexing like this might be better:\n",
    "        # data_assist.iloc[sample_sel][np.append(f_sel_ind[i],feature_t)]\n",
    "        # We want to include the column of f_t_ind into our data_train\n",
    "#         data_train = data_assist.iloc[sample_sel, f_sel_ind[i] + [f_t_ind]]\n",
    "        data_train = data_assist.iloc[sample_sel][np.append(f_sel_ind[i],feature_t)]\n",
    "        colm_t = data_train.apply(lambda x: x.mean(), axis=1)\n",
    "        data_train = data_train - np.outer(np.ones(data_train.shape[1]), colm_t).T\n",
    "        data_train.columns = np.append(f_sel_ind[i], \"target\")\n",
    "        ###get decision tree and RMSE\n",
    "        # import xgboost as xgb\n",
    "        # Fit the model to the data (handles missing values)\n",
    "        # tree.append(xgb.XGBRegressor(max_depth=5, min_child_weight=5))\n",
    "        # tree[i].fit(data_train.iloc[:, :-1], data_train[\"target\"]) # Sometimes runs into nan somehow... ACTUALLY just for the outdated version of sklearn, new is fine\n",
    "        tree.append(DecisionTreeRegressor(min_samples_split=5))\n",
    "        tree[i].fit(data_train.iloc[:, :-1], data_train[\"target\"])\n",
    "        # Compile testing dataset\n",
    "        data_test = data_assist.drop([data_assist.index[idx] for idx in sample_sel], axis=0)[np.append(f_sel_ind[i], feature_t)]\n",
    "        colm_t = data_test.apply(lambda x: x.mean(), axis=1)\n",
    "        data_test = data_test - np.outer(np.ones(data_test.shape[1]), colm_t).T\n",
    "\n",
    "        pred_t = tree[i].predict(data_test.iloc[:, :-1])\n",
    "        RMSE.append(np.sqrt(np.mean((pred_t - data_test[feature_t]) ** 2)))\n",
    "        \n",
    "    ###predict in the target dataset\n",
    "    pred_list = np.zeros((data_t.shape[0], n_best_tree))\n",
    "    for i in range(n_best_tree):\n",
    "        j = np.argsort(RMSE)\n",
    "        f_t_ind = data_t.columns.get_loc(feature_t)\n",
    "        f_t_sel_ind = [data_t.columns.get_loc(f) for f in f_sel_ind[j[i]]]\n",
    "        data_test = data_t.iloc[:, np.append(f_t_sel_ind, f_t_ind)]\n",
    "#         data_test = data_t.iloc[:, f_t_sel_ind + [f_t_ind]]\n",
    "        colm_t = data_test.apply(lambda x: x.mean(), axis=1)\n",
    "        data_test = data_test - np.outer(np.ones(data_test.shape[1]), colm_t).T\n",
    "        data_test.columns = np.append(f_sel_ind[j[i]], \"target\")\n",
    "#         data_test.columns = f_sel_ind[j[i]] + [\"target\"] # hmmm...\n",
    "\n",
    "        pred_t = tree[j[i]].predict(data_test.iloc[:, :-1])\n",
    "        pred_t[np.where(data_test.iloc[:, :-1].isna().sum(axis=1) > 0)] = np.nan\n",
    "        pred_list[:,i]=pred_t\n",
    "\n",
    "    # Obtain predictions and errors\n",
    "    true_err = np.sqrt(np.nanmean((pred_list - data_test['target'].values.reshape(-1, 1))**2, axis=0)) # Changed from axis  1 to 0 for taking mean :-/\n",
    "    argSortedRMSE = [RMSE[idx] for idx in np.argsort(RMSE)]\n",
    "    pred_err = argSortedRMSE[:n_best_tree]\n",
    "    mu = np.nanmean(pred_list, axis=1)\n",
    "    return {'mu': mu, 'true_err': true_err, 'pred_err': pred_err}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0d9ac907",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9383044d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_trivialize_df(df):\n",
    "    # Search columns for trivial feature (virus)\n",
    "    dropped = 0\n",
    "    for idx, col in enumerate(df.columns):\n",
    "        uniques = df[col].unique()\n",
    "        num_unique = len(uniques)\n",
    "        if num_unique == 1 and 'sera_table' not in col and \"*\" in uniques: # Critereon for a missing virus\n",
    "#             print(idx, col, num_unique, uniques)\n",
    "            df = df.drop(columns=col)\n",
    "            dropped += 1 # Count dropped virus\n",
    "    # Search rows for trivial entry (serum)\n",
    "    for idx in list(df.index):\n",
    "        num_unique = len(df.loc[idx].unique())\n",
    "        if num_unique == 3: # Serum name and table name are 2 constant features.\n",
    "#             print(idx, num_unique)\n",
    "            df = df.drop(index=idx)\n",
    "    df = df.replace(\"*\", np.nan)\n",
    "    df = df.set_index('Measurements (Sera in Rows/Viruses in Columns)')\n",
    "    df = df.drop(['sera_table'], axis=1)\n",
    "    df = df.applymap(lambda x: pd.to_numeric(x, errors='coerce') ) # Might wanna just do df.map\n",
    "    # Return data and number of dropped viruses\n",
    "    return df, dropped\n",
    "\n",
    "\n",
    "def plot_heatmap(df):\n",
    "    mask = df.isnull()\n",
    "    g = sns.heatmap(df, cmap='gray',mask=mask)\n",
    "    g.set_facecolor('xkcd:salmon')\n",
    "    return g\n",
    "\n",
    "\n",
    "def plot_heatmap_subplot(table_dict):\n",
    "#     keys = sorted(list(table_dict.keys()))\n",
    "    keys = [\"TableS1\", \"TableS3\", \"TableS5\", \"TableS6\", \"TableS13\", \"TableS14\"]\n",
    "    N = len(keys)\n",
    "    fig, axs = plt.subplots(nrows=1, ncols=N, figsize=(30, 5))\n",
    "    for idx, key in enumerate(keys):\n",
    "        df, dropped_viruses = table_dict[key]\n",
    "        percent_missing = sum(list(df.isna().sum())) / (df.shape[0] * df.shape[1]) # Proportion of missing vals in table\n",
    "        df = np.log(df)\n",
    "        sera_num = df.shape[0]\n",
    "        virus_num = df.shape[1]\n",
    "        mask = df.isnull()\n",
    "        g = sns.heatmap(df,\n",
    "                        xticklabels=False,\n",
    "                        yticklabels=False,\n",
    "                        cmap='gray',\n",
    "                        mask=mask,\n",
    "                        ax=axs[idx])\n",
    "        g.set_facecolor('xkcd:salmon')\n",
    "        axs[idx].set_xlabel(f'{sera_num} Sera x {virus_num} Viruses\\n{percent_missing * 100:.2f}% Missing')\n",
    "        axs[idx].set_title(key)\n",
    "    return fig, axs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "61e9d5f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_573/759662303.py:20: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(lambda x: pd.to_numeric(x, errors='coerce') ) # Might wanna just do df.map\n",
      "/tmp/ipykernel_573/759662303.py:20: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(lambda x: pd.to_numeric(x, errors='coerce') ) # Might wanna just do df.map\n",
      "/tmp/ipykernel_573/759662303.py:20: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(lambda x: pd.to_numeric(x, errors='coerce') ) # Might wanna just do df.map\n",
      "/tmp/ipykernel_573/759662303.py:20: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(lambda x: pd.to_numeric(x, errors='coerce') ) # Might wanna just do df.map\n",
      "/tmp/ipykernel_573/759662303.py:20: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(lambda x: pd.to_numeric(x, errors='coerce') ) # Might wanna just do df.map\n",
      "/tmp/ipykernel_573/759662303.py:20: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(lambda x: pd.to_numeric(x, errors='coerce') ) # Might wanna just do df.map\n",
      "/tmp/ipykernel_573/2568677591.py:23: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  data_df = data_df.applymap(lambda x: pd.to_numeric(x, errors='coerce') )\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "data_t: DF of sera (table specific) X Viruses (No viruses dropped yet) to make prediction on\n",
    "data_s_list: list of columns of data table for virus to train on ... Its the whole df I think\n",
    "feature_t: A particular virus to predict (And I assume we omit this from training)\n",
    "n_tree=50\n",
    "n_feature=5\n",
    "f_sample=0.3\n",
    "n_best_tree=5\n",
    "'''\n",
    "\n",
    "flu_df = pd.read_csv(\"../CrossStudyCompletion/Matrix Completion in R/InfluenzaData.csv\", sep=',')\n",
    "sera = flu_df['Measurements (Sera in Rows/Viruses in Columns)'].tolist()\n",
    "sera_tables = [i[i.index('Table'):] for i in sera]\n",
    "table_keys = set(sera_tables)\n",
    "flu_df['sera_table'] = sera_tables\n",
    "\n",
    "flu_table_dict = {table_key: flu_df.loc[flu_df['sera_table']==table_key] for table_key in table_keys}\n",
    "filtered_flu_table_dict = {key: non_trivialize_df(arg) for key, arg in list(flu_table_dict.items())}\n",
    "\n",
    "predict_table = 'TableS14'\n",
    "train_table = 'TableS13'\n",
    "data_df = flu_df.set_index(\"Measurements (Sera in Rows/Viruses in Columns)\")\n",
    "data_df = data_df.applymap(lambda x: pd.to_numeric(x, errors='coerce') )\n",
    "data_df = np.log10(data_df)\n",
    "data_df['sera_table'] = sera_tables\n",
    "\n",
    "data_t = data_df.loc[data_df['sera_table']==predict_table].drop(['sera_table'], axis=1)\n",
    "data_s = data_df.loc[data_df['sera_table']==train_table].drop(['sera_table'], axis=1)\n",
    "data_s_list = [data_s] # [[col] for col in list(data_s.columns)]\n",
    "feature_t = \"A/PANAMA/2007/99\"\n",
    "\n",
    "# K = len(data_s) # 160\n",
    "# data_assist = data_s\n",
    "# f_t_ind = list(data_assist.columns).index(feature_t)\n",
    "# data_t.columns.equals(data_t.columns)\n",
    "# f_t_ind = data_assist.columns.get_loc(feature_t)\n",
    "n_tree=50\n",
    "n_feature=5\n",
    "f_sample=0.3\n",
    "n_best_tree=5\n",
    "k=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "53de176d",
   "metadata": {
    "cell_style": "center",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 additional datasets used for prediction.\n",
      "Feature-of-interest located!\n",
      "data_assist shape: (160, 81)\n",
      "Iterating over 81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_573/3077283591.py:169: RuntimeWarning: Mean of empty slice\n",
      "  mu = np.nanmean(pred_list, axis=1)\n",
      "/tmp/ipykernel_573/3077283591.py:169: RuntimeWarning: Mean of empty slice\n",
      "  mu = np.nanmean(pred_list, axis=1)\n",
      "/tmp/ipykernel_573/3077283591.py:169: RuntimeWarning: Mean of empty slice\n",
      "  mu = np.nanmean(pred_list, axis=1)\n",
      "/tmp/ipykernel_573/3077283591.py:169: RuntimeWarning: Mean of empty slice\n",
      "  mu = np.nanmean(pred_list, axis=1)\n",
      "/tmp/ipykernel_573/3077283591.py:169: RuntimeWarning: Mean of empty slice\n",
      "  mu = np.nanmean(pred_list, axis=1)\n",
      "/tmp/ipykernel_573/3077283591.py:169: RuntimeWarning: Mean of empty slice\n",
      "  mu = np.nanmean(pred_list, axis=1)\n",
      "/tmp/ipykernel_573/3077283591.py:169: RuntimeWarning: Mean of empty slice\n",
      "  mu = np.nanmean(pred_list, axis=1)\n",
      "/tmp/ipykernel_573/3077283591.py:169: RuntimeWarning: Mean of empty slice\n",
      "  mu = np.nanmean(pred_list, axis=1)\n",
      "/tmp/ipykernel_573/3077283591.py:169: RuntimeWarning: Mean of empty slice\n",
      "  mu = np.nanmean(pred_list, axis=1)\n",
      "/tmp/ipykernel_573/3077283591.py:169: RuntimeWarning: Mean of empty slice\n",
      "  mu = np.nanmean(pred_list, axis=1)\n",
      "/tmp/ipykernel_573/3077283591.py:169: RuntimeWarning: Mean of empty slice\n",
      "  mu = np.nanmean(pred_list, axis=1)\n",
      "/tmp/ipykernel_573/3077283591.py:169: RuntimeWarning: Mean of empty slice\n",
      "  mu = np.nanmean(pred_list, axis=1)\n",
      "/tmp/ipykernel_573/3077283591.py:169: RuntimeWarning: Mean of empty slice\n",
      "  mu = np.nanmean(pred_list, axis=1)\n",
      "/tmp/ipykernel_573/3077283591.py:169: RuntimeWarning: Mean of empty slice\n",
      "  mu = np.nanmean(pred_list, axis=1)\n",
      "/tmp/ipykernel_573/3077283591.py:169: RuntimeWarning: Mean of empty slice\n",
      "  mu = np.nanmean(pred_list, axis=1)\n",
      "/tmp/ipykernel_573/3077283591.py:169: RuntimeWarning: Mean of empty slice\n",
      "  mu = np.nanmean(pred_list, axis=1)\n",
      "/tmp/ipykernel_573/3077283591.py:169: RuntimeWarning: Mean of empty slice\n",
      "  mu = np.nanmean(pred_list, axis=1)\n",
      "/tmp/ipykernel_573/3077283591.py:169: RuntimeWarning: Mean of empty slice\n",
      "  mu = np.nanmean(pred_list, axis=1)\n",
      "/tmp/ipykernel_573/3077283591.py:169: RuntimeWarning: Mean of empty slice\n",
      "  mu = np.nanmean(pred_list, axis=1)\n",
      "/tmp/ipykernel_573/3077283591.py:169: RuntimeWarning: Mean of empty slice\n",
      "  mu = np.nanmean(pred_list, axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing lm stuff\n",
      "['a=0.886', 'b=0.097', 'c=0.05']\n",
      "rf_1t1: {'mu': array([-0.06522317,  0.07024033, -0.07024033,  0.2257725 ,  0.03762875,\n",
      "        0.07358511,  0.12710155,  0.18396278, -0.0451545 ,  0.07692989,\n",
      "        0.0894728 ,  0.19399711, -0.10619669, -0.12542916,  0.21239339,\n",
      "        0.03595636,  0.36553642, -0.09950714,  0.20486764, -0.09867094,\n",
      "        0.10285192,  0.18229039, -0.02257725,  0.14884261, -0.09198139,\n",
      "        0.30270238,  0.26925461,  0.25922027,  0.26590983,  0.2859785 ,\n",
      "       -0.19065233,  0.41809722,  0.12710155, -0.14800641,  0.13379111,\n",
      "        0.01923247, -0.07024033,  0.08194705,  0.15553216, -0.11037767,\n",
      "       -0.07692989,  0.13211872,  0.20403144, -0.11037767,  0.17727322,\n",
      "        0.23078966, -0.08529183,  0.23162586,  0.56359505,  0.24249639,\n",
      "        0.07776608,  0.20570383,  0.11539483,  0.25671169,  0.10297137,\n",
      "               nan,  0.22911727, -0.11037767, -0.09532617,  0.29195131,\n",
      "       -0.05351644,  0.12124819, -0.08194705,  0.41570809, -0.09783475,\n",
      "        0.19734189,  0.34451211,  0.40639049,  0.08696422,  0.16640269,\n",
      "        0.17058366, -0.060206  , -0.16138553,  0.1956695 , -0.060206  ,\n",
      "        0.49168233,  0.51509577,  0.22660869,  0.00501717,  0.36792555,\n",
      "        0.27259938,  0.11037767,  0.0752575 ,  0.32109866,  0.06187839,\n",
      "        0.16640269,  0.180618  ,  0.14382544, -0.03177539, -0.0593698 ,\n",
      "        0.180618  ,  0.17894561, -0.12542916, -0.11288625,  0.47161366,\n",
      "        0.08445564,  0.26590983,  0.03762875,  0.25838408,  0.11037767,\n",
      "        0.16891128,  0.27427177, -0.12292058,  0.31273672,  0.0752575 ,\n",
      "        0.18897994,  0.24918594,  0.25922027,  0.23078966, -0.03512017,\n",
      "        0.00585336,  0.33196919,  0.21239339, -0.04348211,  0.13379111,\n",
      "        0.06438697,  0.37389837,  0.20068666,  0.00501717, -0.14215305,\n",
      "        0.08696422,  0.210721  , -0.03679256,  0.00418097,  0.08445564,\n",
      "        0.22326391,  0.17894561,  0.26172886,  0.55188833,  0.3688812 ,\n",
      "        0.24416877,  0.24249639,  0.07024033,  0.08612803,  0.16819454,\n",
      "               nan,  0.29935761, -0.180618  ,  0.03930114,  0.18229039,\n",
      "        0.04682689, -0.00752575,  0.04766308,  0.2859785 , -0.01337911,\n",
      "        0.3688812 ,  0.30437477,  0.2558755 ,  0.1956695 ,  0.26770167,\n",
      "        0.11372244,  0.1655665 ,  0.04766308,  0.26758222,  0.210721  ,\n",
      "        0.43649349,  0.49168233,  0.35705502,  0.30103   ,  0.40973527]), 'true_err': array([0.26221139, 0.242692  , 0.2600613 , 0.32607423, 0.24897604]), 'pred_err': [0.14218697215641177, 0.19585584554825572, 0.20305795256909664, 0.21616995325233107, 0.21834130607613494]}\n",
      "len sigma: 1\n",
      "[0.3199378632304584]\n",
      "K: 1\n",
      "k: 0\n",
      "1 assisting data used for prediction.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_573/3077283591.py:169: RuntimeWarning: Mean of empty slice\n",
      "  mu = np.nanmean(pred_list, axis=1)\n"
     ]
    }
   ],
   "source": [
    "results = RF_complete(data_t = data_t,\n",
    "                      data_s_list=[data_s],\n",
    "                      feature_t=feature_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "af46324b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'predictions': array([-0.06522317,  0.07024033, -0.07024033,  0.2257725 ,  0.03762875,\n",
       "         0.07358511,  0.12710155,  0.18396278, -0.0451545 ,  0.07692989,\n",
       "         0.0894728 ,  0.19399711, -0.10619669, -0.12542916,  0.21239339,\n",
       "         0.03595636,  0.36553642, -0.09950714,  0.20486764, -0.09867094,\n",
       "         0.10285192,  0.18229039, -0.02257725,  0.14884261, -0.09198139,\n",
       "         0.30270238,  0.26925461,  0.25922027,  0.26590983,  0.2859785 ,\n",
       "        -0.19065233,  0.41809722,  0.12710155, -0.14800641,  0.13379111,\n",
       "         0.01923247, -0.07024033,  0.08194705,  0.15553216, -0.11037767,\n",
       "        -0.07692989,  0.13211872,  0.20403144, -0.11037767,  0.17727322,\n",
       "         0.23078966, -0.08529183,  0.23162586,  0.56359505,  0.24249639,\n",
       "         0.07776608,  0.20570383,  0.11539483,  0.25671169,  0.10297137,\n",
       "                nan,  0.22911727, -0.11037767, -0.09532617,  0.29195131,\n",
       "        -0.05351644,  0.12124819, -0.08194705,  0.41570809, -0.09783475,\n",
       "         0.19734189,  0.34451211,  0.40639049,  0.08696422,  0.16640269,\n",
       "         0.17058366, -0.060206  , -0.16138553,  0.1956695 , -0.060206  ,\n",
       "         0.49168233,  0.51509577,  0.22660869,  0.00501717,  0.36792555,\n",
       "         0.27259938,  0.11037767,  0.0752575 ,  0.32109866,  0.06187839,\n",
       "         0.16640269,  0.180618  ,  0.14382544, -0.03177539, -0.0593698 ,\n",
       "         0.180618  ,  0.17894561, -0.12542916, -0.11288625,  0.47161366,\n",
       "         0.08445564,  0.26590983,  0.03762875,  0.25838408,  0.11037767,\n",
       "         0.16891128,  0.27427177, -0.12292058,  0.31273672,  0.0752575 ,\n",
       "         0.18897994,  0.24918594,  0.25922027,  0.23078966, -0.03512017,\n",
       "         0.00585336,  0.33196919,  0.21239339, -0.04348211,  0.13379111,\n",
       "         0.06438697,  0.37389837,  0.20068666,  0.00501717, -0.14215305,\n",
       "         0.08696422,  0.210721  , -0.03679256,  0.00418097,  0.08445564,\n",
       "         0.22326391,  0.17894561,  0.26172886,  0.55188833,  0.3688812 ,\n",
       "         0.24416877,  0.24249639,  0.07024033,  0.08612803,  0.16819454,\n",
       "                nan,  0.29935761, -0.180618  ,  0.03930114,  0.18229039,\n",
       "         0.04682689, -0.00752575,  0.04766308,  0.2859785 , -0.01337911,\n",
       "         0.3688812 ,  0.30437477,  0.2558755 ,  0.1956695 ,  0.26770167,\n",
       "         0.11372244,  0.1655665 ,  0.04766308,  0.26758222,  0.210721  ,\n",
       "         0.43649349,  0.49168233,  0.35705502,  0.30103   ,  0.40973527]),\n",
       " 'errors': 0.3199378632304584}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (datasci)",
   "language": "python",
   "name": "dsci"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 501.65,
   "position": {
    "height": "401.65px",
    "left": "161px",
    "right": "20px",
    "top": "244px",
    "width": "712.2px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
