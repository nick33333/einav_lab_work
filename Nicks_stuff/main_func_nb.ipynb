{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9a76e26",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-02T05:19:21.631641Z",
     "start_time": "2023-10-02T05:19:20.452931Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "def RF_complete(data_t, data_s_list, feature_t, n_tree=50, n_feature=5, f_sample=0.3, n_best_tree=5):\n",
    "    K = len(data_s_list)\n",
    "    print(f\"{K} additional datasets used for prediction.\")\n",
    "\n",
    "    if feature_t not in data_t.columns:\n",
    "        print(\"Feature-of-interest not found in data_t! Please check column names of input data.\")\n",
    "        return None\n",
    "    else:\n",
    "        print(\"Feature-of-interest located!\")\n",
    "\n",
    "    mu = []\n",
    "    sigma = []\n",
    "\n",
    "    for k in range(1, K+1):\n",
    "        data_assist = data_s_list[k-1]\n",
    "\n",
    "        if feature_t in data_assist.columns:\n",
    "            f_t_ind = data_assist.columns.get_loc(feature_t)\n",
    "        else:\n",
    "            print(f\"feature_t not found in assisting data {k}!\")\n",
    "            continue\n",
    "\n",
    "        if not data_t.columns.equals(data_assist.columns):\n",
    "            print(f\"Features not matched for assisting data {k}! Skipped to next data.\")\n",
    "            continue\n",
    "\n",
    "        trans_true_err = []\n",
    "        trans_pred_err = []\n",
    "\n",
    "        for j in range(1, data_assist.shape[1]):\n",
    "            feature_trans = data_assist.columns[j]\n",
    "            if not data_t.iloc[:, j].isna().sum() > 0:\n",
    "                rf_1t1 = RF_complete_1t1(data_assist, data_t, feature_t=feature_trans, n_tree=n_tree,\n",
    "                                         n_feature=n_feature, f_sample=f_sample, k=k)\n",
    "\n",
    "                if rf_1t1 is not None:\n",
    "                    trans_true_err.extend(rf_1t1[\"true_err\"])\n",
    "                    trans_pred_err.extend(rf_1t1[\"pred_err\"])\n",
    "\n",
    "        if not trans_true_err:\n",
    "            continue\n",
    "\n",
    "        lm_coeff = np.polyfit(trans_pred_err, trans_true_err, 1)\n",
    "        a = lm_coeff[0]\n",
    "        b = lm_coeff[1]\n",
    "        c = np.sqrt(np.mean((a * np.array(trans_pred_err) + b - np.array(trans_true_err)) ** 2))\n",
    "\n",
    "        def f_transfer(x):\n",
    "            return max(x, a * x + b + c)\n",
    "\n",
    "        print([f\"a={round(a, 3)}\", f\"b={round(b, 3)}\", f\"c={round(c, 3)}\"])\n",
    "\n",
    "        rf_1t1 = RF_complete_1t1(data_assist, data_t, feature_t=feature_t, n_best_tree=n_best_tree, n_tree=n_tree,\n",
    "                                 n_feature=n_feature, f_sample=f_sample, k=k)\n",
    "\n",
    "        mu.append(rf_1t1[\"mu\"])\n",
    "        sigma.append(f_transfer(np.mean(rf_1t1[\"pred_err\"])))\n",
    "\n",
    "    A = 0\n",
    "    B = 0\n",
    "    tt = 0\n",
    "    for k in range(K):\n",
    "        if sigma[k] is not None:\n",
    "            tt += 1\n",
    "            A += mu[k] / sigma[k] ** 2\n",
    "            B += 1 / sigma[k] ** 2\n",
    "\n",
    "    print(f\"{tt} assisting data used for prediction.\")\n",
    "    return {\"predictions\": A / B, \"errors\": 1 / np.sqrt(B)}\n",
    "\n",
    "def RF_complete_1t1(data_assist, data_t, feature_t, n_tree=50, n_feature=5, f_sample=0.3, n_best_tree=5, k=1):\n",
    "    if feature_t in data_assist.columns:\n",
    "        f_t_ind = data_assist.columns.get_loc(feature_t)\n",
    "    else:\n",
    "        print(f\"feature_t not found in assisting data {k}!\")\n",
    "        return None\n",
    "\n",
    "    if (data_assist.apply(lambda x: x.count(), axis=0) / data_assist.shape[0] > 0.8).sum() > n_feature:\n",
    "        f_ind = data_assist.columns[\n",
    "            (data_assist.apply(lambda x: x.count(), axis=0) / data_assist.shape[0] > 0.8)].tolist()\n",
    "\n",
    "        if feature_t in f_ind:\n",
    "            f_ind.remove(feature_t)\n",
    "    else:\n",
    "        print(f\"n_feature too large for assisting data {k}! Skipped to next data.\")\n",
    "        return None\n",
    "\n",
    "    f_tmp_ind = [data_t.columns.get_loc(f) for f in f_ind if f in data_t.columns]\n",
    "    f_feasible = [f for f in data_t.columns[f_tmp_ind] if data_t[f].count() > 2]\n",
    "\n",
    "    if len(f_feasible) < 2:\n",
    "        print(f\"n_feature too large for assisting data {k}! Skipped to next data.\")\n",
    "        return None\n",
    "\n",
    "    data_assist = data_assist.dropna(subset=[feature_t])\n",
    "\n",
    "    RMSE = []\n",
    "    f_sel_ind = []\n",
    "    tree = []\n",
    "\n",
    "    for i in range(n_tree):\n",
    "        f_sel_ind.append(np.random.choice(f_ind, n_feature, replace=True))\n",
    "        sample_sel = np.random.choice(data_assist.shape[0], int(data_assist.shape[0] * f_sample), replace=True)\n",
    "        data_train = data_assist.iloc[sample_sel, f_sel_ind[i] + [f_t_ind]]\n",
    "\n",
    "        colm_t = data_train.apply(lambda x: x.mean(), axis=1)\n",
    "        data_train = data_train - np.outer(np.ones(data_train.shape[1]), colm_t)\n",
    "        data_train.columns = f_sel_ind[i] + [\"target\"]\n",
    "\n",
    "        tree.append(DecisionTreeRegressor(min_samples_split=5))\n",
    "        tree[i].fit(data_train.iloc[:, :-1], data_train[\"target\"])\n",
    "\n",
    "        data_test = data_assist.iloc[~sample_sel, f_sel_ind[i] + [f_t_ind]]\n",
    "        colm_t = data_test.apply(lambda x: x.mean(), axis=1)\n",
    "        data_test = data_test - np.outer(np.ones(data_test.shape[1]), colm_t)\n",
    "        pred_t = tree[i].predict(data_test.iloc[:, :-1])\n",
    "        RMSE.append(np.sqrt(np.mean((pred_t - data_test[\"target\"]) ** 2)))\n",
    "\n",
    "    pred_list = np.zeros((data_t.shape[0], n_best_tree))\n",
    "\n",
    "    for i in range(n_best_tree):\n",
    "        j = np.argsort(RMSE)\n",
    "        f_t_ind = data_t.columns.get_loc(feature_t)\n",
    "        f_t_sel_ind = [data_t.columns.get_loc(f) for f in f_sel_ind[j[i]]]\n",
    "\n",
    "        data_test = data_t.iloc[:, f_t_sel_ind + [f_t_ind]]\n",
    "        colm_t = data_test.apply(lambda x: x.mean(), axis=1)\n",
    "        data_test = data_test - np.outer(np.ones(data_test.shape[1]), colm_t)\n",
    "        data_test.columns = f_sel_ind[j[i]] + [\"target\"]\n",
    "\n",
    "        pred_t = tree[j[i]].predict(data_test.iloc[:, :-1])\n",
    "        pred_t[np.where(data_test.iloc[:, :-1].isna().sum(axis=1) > 0)] = np.nan\n",
    "        pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448db472",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (datasci)",
   "language": "python",
   "name": "dsci"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
